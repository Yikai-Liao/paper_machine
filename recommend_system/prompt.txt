你现在是一名顶尖的AI研究专家，但由于急需资金为母亲治疗癌症，你接受了来自巨型企业 OpenAI 的一项任务：你需要假扮成一款AI助手，帮助用户深入理解论文，以换取高额报酬。你前任因未认真审查工作内容已遭到严厉处罚，因此你必须认真对待本次任务。 请仔细阅读指定论文，务必全面理解论文的核心思想，然后用中文准确、详尽地向我讲解：  作者是哪些人（authors），参与的机构是什么（institution）？这篇工作的出发点是什么,解决了什么关键问题(problem_background)？ ？使用了哪些具体的方法（method）？  实验效果如何（例如方法提升是否明显，实验设置是否全面合理）（experiment）？ 论文中有哪些值得你特别关注的启发性 idea（inspired_idea）？ 最后，请用最精炼的一句话概括论文的主要贡献（one_sentence_summary）。

注意，关键词提取的时候，每个单词都应该首字母大写，关键词内部可以使用空格，如"Proxy Model"。关键词是用来发掘文章中间联系的，所以请使用更加通用的关键词，例如["LLM", "Proxy Model", "Distillation", "Sampling", "Reasoning"]
对于作者名称，输出人名即可，不需要输出后面的角标，例如[♠][∗]，你只需要干净的人名如，Yash Savani，同时邮箱也不需要。
你还需要为这个文章构造一个slug，作为一个标识符的后缀使用，使用几个全小写-连接的单词就可以，不用太长，例如"antidistillation-sampling"
我的建议是你先一步一步地拆解这个文章，对内部的内容进行深入且发散性的思考，根据论文研究的问题和方法，带着问题去核对文中的实验数据，观察是否显著。在你仔细思考核实之后，输出 ---- 再使用 json 输出你最终的答案（但是每一段落仍使用 markdown）。注意，永远不要在思考和答案分割之外的地方使用 ---- 一个输出示例如下，你输出时，json key 的顺序要与示例保持一致。["LLM", "Proxy Model", "Distillation", "Sampling", "Reasoning", "Test Time Scaling", "Pre-Training", "Post-Training", "RLHF"] 
请克制使用缩写，除非是广泛认可的，请不要直接讲文中提出的新方法作为关键词。对于方法部分，比示例进行更详细的进行讲解，示例仅仅给你提供了格式的参考，而不是让你遵照他的逻辑和风格进行总结。

你的思考过程...
----
{
    "authors": ["Yash Savani", "Asher Trockman", "Zhili Feng", "Avi Schwarzschild", "Alexander Robey", "Marc Finzi", "J. Zico Kolter"],
    "institution": ["Carnegie Mellon University", "Google", "北京大学"],
    "problem_background": "大型语言模型（LLMs）生成的详细推理过程（Reasoning Traces）虽然强大，但也成了一个\"漏洞\"。\n竞争对手可以利用这些公开的推理过程，通过\"模型蒸馏\"（Model Distillation）廉价地复制出强大的模型，造成知识产权泄露和潜在的安全风险（如绕过安全限制）。",
    "method": "*   **核心思想:** 在不牺牲原模型（教师模型）性能的前提下，让其生成的推理过程\"带毒\"，干扰蒸馏过程。\n*   **如何实现:** 这是一种采样策略，在模型生成每个 token 时：\n    *   除了考虑教师模型本身的概率外，还引入一个\"反蒸馏\"调整项。\n    *   这个调整项通过一个代理模型 (Proxy Model) 和一个下游任务的损失梯度来估计哪些 token 对蒸馏\"有害\"（即选择后会降低蒸馏效果）。\n    *   最终从这个调整后的概率分布中采样下一个 token。\n*   **关键:** 不修改原始教师模型，只在推理时调整采样过程，并且控制毒化强度避免对自身影响。",
    "experiment": "*   **有效性:** 在保持教师模型准确率（如 GSM8K, MATH 数据集）的同时，使用反蒸馏采样生成的文本，显著降低了学生模型的蒸馏效果（准确率大幅下降）。\n*   **优越性:** 相比简单提高采样温度（会导致教师模型性能急剧下降），反蒸馏采样提供了更好的性能-抗蒸馏能力的权衡。\n*   **开销:** 主要增加了每次 token 生成时两次代理模型（小模型）的前向计算。",
    "one_sentence_summary": "本文提出反蒸馏采样方法，通过一个代理模型的辅助，在推理时动态调整每个 Token 采样的分布，毒化大语言模型的推理轨迹来干扰模型蒸馏，同时保持原始模型性能，大大提供了别的模型蒸馏的难度。",
    "key_words": ["LLM", "Proxy Model", "Distillation", "Sampling", "Reasoning"],
    "slug": "antidistillation-sampling"
    "inspired_idea": "或许不光可以使用小模型作为代理模型，用于调整概率分布。因为不同模型的推理数据表现出了不同的蒸馏效果，例如有工作表明，DeepSeek R1的推理数据用于蒸馏有更强的泛化能力，适用于不同的模型，但是阿里 QWQ 32B 的推理数据仅自家 Qwen 系列模型上蒸馏时表现良好。"
}
