{
    "authors": ["Xiaoxi Li", "Jiajie Jin", "Guanting Dong", "Hongjin Qian", "Yutao Zhu", "Yongkang Wu", "Ji-Rong Wen", "Zhicheng Dou"],
    "institution": ["Renmin University of China", "BAAI", "Huawei Poisson Lab"],
    "problem_background": "**Problem Background:** Large reasoning models (LRMs) such as OpenAI-o1 and DeepSeek-R1 excel in long-horizon reasoning tasks but are limited by their reliance on static internal knowledge. This restricts their performance on complex, knowledge-intensive tasks that require synthesizing diverse web information, such as generating comprehensive research reports, and hinders their ability to conduct in-depth web exploration and real-time information gathering.",
    "method": "**Method:** WebThinker is a framework that enhances LRMs with deep research capabilities through three main components: (1) **Deep Web Explorer**, which allows LRMs to autonomously search the web, navigate pages by clicking links or buttons, and extract relevant information to address knowledge gaps; (2) **Autonomous Think-Search-and-Draft strategy**, enabling LRMs to interleave reasoning, information gathering, and report writing in real time, using tools for drafting, checking, and editing report sections; and (3) **RL-based training strategy**, which employs iterative online Direct Preference Optimization (DPO) to improve tool utilization by generating preference pairs from reasoning trajectories and optimizing the model's policy. The generation process is formalized as \( P(R, y \mid I, q, T) = \prod_{t=1}^{T_r} P(R_t \mid R_{<t}, I, q, \{O_\tau\}_{\tau<t}) \cdot \prod_{t=1}^{T_y} P(y_t \mid y_{<t}, R, I, q) \), where \( R \) is the reasoning chain, \( y \) is the output, \( I \) is the instruction, \( q \) is the query, \( T \) is the tool set, and \( O_\tau \) represents tool outputs, ensuring seamless integration of external knowledge retrieval and reasoning.",
    "experiment": "**Experiment:** WebThinker was evaluated on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive). Results show significant improvements, with WebThinker-32B outperforming baselines like Search-o1 by 22.9% on WebWalkerQA and 20.4% on HLE, while maintaining high accuracy on GPQA (e.g., 68.7% average). The RL-trained version further enhanced performance by 8.5% on GAIA and 21.5% on HLE. Experimental settings were comprehensive, using diverse datasets and metrics like Pass@1, with ablation studies confirming the importance of components like Deep Web Explorer (performance drop to 38.3% average without it). The setup is reasonable, leveraging open-source models and real-world web interactions, though it relies on external APIs, which may introduce variability.",
    "one_sentence_summary": "WebThinker introduces a framework that equips large reasoning models with autonomous web exploration and report generation capabilities through integrated modules and reinforcement learning, significantly enhancing their performance on knowledge-intensive tasks.",
    "key_words": ["LLM", "RAG", "Reinforcement Learning", "Web Search", "Report Generation"],
    "slug": "webthinker-deep-research",
    "inspired_idea": "The RL-based training for tool utilization in WebThinker could inspire the development of more adaptive AI agents that dynamically learn from interactions, potentially extending to multimodal data integration or advanced GUI-based web exploration to handle non-textual information and broader real-world scenarios."
}